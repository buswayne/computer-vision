# -*- coding: utf-8 -*-
"""BUSETTO_Assignment_4.ipynb

Automatically generated by Colaboratory.

### INITIALIZATION
"""

from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

# Hyperparameters
device = torch.device('cuda')
num_epochs = 3
num_classes = 10
learning_rate = 0.001
input_dimension = 1
output_dimension = 10

# LOAD DATASET
from   torchvision import datasets as datasets
import torchvision.transforms as transforms
import torch.utils as utils
import matplotlib.pyplot as plt
import torchvision

trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = datasets.MNIST('./data', train=True,  download=True, transform=trans)
test_dataset  = datasets.MNIST('./data', train=False, download=True, transform=trans)
train_loader = utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=2)
test_loader  = utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2)

# Script for testing a network
def evaluate_model(net, dataloader, tag, Pretrained=None, resize_to=None):
    print("\nTesting the network...")
    if not Pretrained is None:
        net.load_state_dict(torch.load(Pretrained))
    net.eval()
    total_num   = 0
    correct_num = 0
    for test_iter, test_data in enumerate(dataloader):
        # Get one batch of test samples
        inputs, labels = test_data    
        if not resize_to is None:
            inputs = F.interpolate(inputs, size=resize_to, mode='bicubic')
        bch = inputs.size(0)

        # Move inputs and labels into GPU
        inputs = inputs.cuda()
        labels = torch.LongTensor(list(labels)).cuda()

        # Forward
        outputs = net(inputs)   

        # Get predicted classes
        _, pred_cls = torch.max(outputs, 1)
#        if total_num == 0:
#           print("True label:\n", labels)
#           print("Prediction:\n", pred_cls)
        # Record test resultplot_traininigloss
        correct_num+= (pred_cls == labels).float().sum().item()
        total_num+= bch
#    net.train()
    #print(tag+" - accuracy: "+"%.4f%\n"%((correct_num/float(total_num))*100))
    print(tag+' Accuracy: {} %\n'.format((correct_num / total_num) * 100))

"""### NET 1: CNN_2fltr_2fcl_ADAM_60000"""

# CREATE THE MODEL
class ConvNet(nn.Module):
  def __init__(self):
    super(ConvNet, self).__init__()
    self.layer1 = nn.Sequential(
      nn.Conv2d(input_dimension, 32, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.layer2 = nn.Sequential(
      nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.drop_out = nn.Dropout()
    self.fc1 = nn.Linear(7 * 7 * 64, 1000)
    self.fc2 = nn.Linear(1000, output_dimension)
    
  def forward(self, x):
    out = self.layer1(x)
    out = self.layer2(out)
    out = out.reshape(out.size(0), -1)     #from 7x7x64 --> 3136x1
    out = self.drop_out(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out

# LOSS FUCTION AND OPTIMIZER
Net_1 = ConvNet().cuda()
import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(Net_1.parameters(), lr=learning_rate)
print(Net_1)

# TRAIN THE MODEL
total_step = len(train_loader)
loss_list = []
acc_list = []
for epoch in range(num_epochs):     # loop over the dataset multiple times
  running_loss = 0.0
  ct_num = 0
  for i, (images, labels) in enumerate(train_loader):
    # Run the forward pass
    images = images.cuda()
    labels = labels.cuda()
    
    # Backprop and perform Adam optimization
    optimizer.zero_grad()
    
    outputs = Net_1(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    loss_list.append(loss.item())
    
    # Print statistics
    running_loss += loss.item()
    ct_num += 1
    if (i + 1) % 100 == 0:
      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
            .format(epoch + 1, num_epochs, i + 1, total_step, running_loss/ct_num))
      
print('Finished training')

"""### NET 2: CNN_2fltr_2fcl_ADAM_60000"""

# CREATE THE MODEL
class ConvNet(nn.Module):
  def __init__(self):
    super(ConvNet, self).__init__()
    self.layer1 = nn.Sequential(
      nn.Conv2d(input_dimension, 32, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.layer2 = nn.Sequential(
      nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.drop_out = nn.Dropout()
    self.fc1 = nn.Linear(7 * 7 * 64, 1000)
    self.fc2 = nn.Linear(1000, output_dimension)
    
  def forward(self, x):
    out = self.layer1(x)
    out = self.layer2(out)
    out = out.reshape(out.size(0), -1)     #from 7x7x64 --> 3136x1
    out = self.drop_out(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out

# LOSS FUCTION AND OPTIMIZER
Net_2 = ConvNet().cuda()
import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(Net_2.parameters(), lr=learning_rate)
print(Net_2)

# TRAIN THE MODEL
total_step = len(train_loader)
loss_list = []
acc_list = []
for epoch in range(num_epochs):     # loop over the dataset multiple times
  running_loss = 0.0
  ct_num = 0
  for i, (images, labels) in enumerate(train_loader):
    # Run the forward pass
    images = images.cuda()
    labels = labels.cuda()
    
    # Backprop and perform Adam optimization
    optimizer.zero_grad()
    
    outputs = Net_2(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    loss_list.append(loss.item())
    
    # Print statistics
    running_loss += loss.item()
    ct_num += 1
    if (i + 1) % 100 == 0:
      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
            .format(epoch + 1, num_epochs, i + 1, total_step, running_loss/ct_num))
      
print('Finished training')

"""### NET 3: CNN_1fltr_2fcl_SGD_60000"""

# CREATE THE MODEL
class ConvNet(nn.Module):
  def __init__(self):
    super(ConvNet, self).__init__()
    self.layer1 = nn.Sequential(
      nn.Conv2d(input_dimension, 32, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.drop_out = nn.Dropout()
    self.fc1 = nn.Linear(14 * 14 * 32, 1000)
    self.fc2 = nn.Linear(1000, output_dimension)
    
  def forward(self, x):
    out = self.layer1(x)
    out = out.reshape(out.size(0), -1)     #from 14x14x32 --> 6272x1
    out = self.drop_out(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out

# LOSS FUCTION AND OPTIMIZER
Net_3 = ConvNet().cuda()
import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(Net_3.parameters(), lr=learning_rate, momentum=0.9)
print(Net_3)

# TRAIN THE MODEL
total_step = len(train_loader)
loss_list = []
acc_list = []
for epoch in range(num_epochs):     # loop over the dataset multiple times
  running_loss = 0.0
  ct_num = 0
  for i, (images, labels) in enumerate(train_loader):
    # Run the forward pass
    images = images.cuda()
    labels = labels.cuda()
    
    # Backprop and perform Adam optimization
    optimizer.zero_grad()
    
    outputs = Net_3(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    loss_list.append(loss.item())
    
    # Print statistics
    running_loss += loss.item()
    ct_num += 1
    if (i + 1) % 100 == 0:
      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
            .format(epoch + 1, num_epochs, i + 1, total_step, running_loss/ct_num))
      
print('Finished training')

"""### NET 4: CNN_1fltr_2fcl_SGD_60000"""

# CREATE THE MODEL
class ConvNet(nn.Module):
  def __init__(self):
    super(ConvNet, self).__init__()
    self.layer1 = nn.Sequential(
      nn.Conv2d(input_dimension, 32, kernel_size=5, stride=1, padding=2),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2, stride=2))
    self.drop_out = nn.Dropout()
    self.fc1 = nn.Linear(14 * 14 * 32, 1000)
    self.fc2 = nn.Linear(1000, output_dimension)
    
  def forward(self, x):
    out = self.layer1(x)
    out = out.reshape(out.size(0), -1)     #from 14x14x32 --> 6272x1
    out = self.drop_out(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out

# LOSS FUCTION AND OPTIMIZER
Net_4 = ConvNet().cuda()
import torch.optim as optim
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(Net_4.parameters(), lr=learning_rate, momentum=0.9)
print(Net_4)

# TRAIN THE MODEL
total_step = len(train_loader)
loss_list = []
acc_list = []
for epoch in range(num_epochs):     # loop over the dataset multiple times
  running_loss = 0.0
  ct_num = 0
  for i, (images, labels) in enumerate(train_loader):
    # Run the forward pass
    images = images.cuda()
    labels = labels.cuda()
    
    # Backprop and perform Adam optimization
    optimizer.zero_grad()
    
    outputs = Net_4(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    loss_list.append(loss.item())
    
    # Print statistics
    running_loss += loss.item()
    ct_num += 1
    if (i + 1) % 100 == 0:
      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
            .format(epoch + 1, num_epochs, i + 1, total_step, running_loss/ct_num))
      
print('Finished training')

"""# FGSM ATTACK: functions"""

# FGSM attack code
def fgsm_attack(image, epsilon, data_grad):
    # Collect the element-wise sign of the data gradient
    sign_data_grad = data_grad.sign()
    # Create the perturbed image by adjusting each pixel of the input image
    perturbed_image = image + epsilon*sign_data_grad
    # Adding clipping to maintain [0,1] range
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image
    return perturbed_image

def test( model, device, test_loader, epsilon ):

    # Accuracy counter
    correct = 0
    adv_examples = []
    adv_samples = []
    sample_labels = []

    # Loop over all examples in test set
    for data, target in test_loader:
    #for _, data in enumerate(test_loader):

        # Send the data and label to the device
        data, target = data.to(device), target.to(device)

        # Set requires_grad attribute of tensor. Important for Attack
        data.requires_grad = True

        # Forward pass the data through the model
        output = model(data)
        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability

        # If the initial prediction is wrong, dont bother attacking, just move on
        if init_pred.item() != target.item():
            continue

        # Calculate the loss
        loss = F.nll_loss(output, target)

        # Zero all existing gradients
        model.zero_grad()

        # Calculate gradients of model in backward pass
        loss.backward()

        # Collect datagrad
        data_grad = data.grad.data

        # Call FGSM Attack
        perturbed_data = fgsm_attack(data, epsilon, data_grad)

        # Save perturbed_images
        adv_samples.append(perturbed_data.detach().cpu())
        sample_labels.append(target)

        # Re-classify the perturbed image
        output = model(perturbed_data)

        # Check for success
        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
        if final_pred.item() == target.item():
            correct += 1
            # Special case for saving 0 epsilon examples
            if (epsilon == 0) and (len(adv_examples) < 5):
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )
        else:
            # Save some adv examples for visualization later
            if len(adv_examples) < 5:
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )

    # Calculate final accuracy for this epsilon
    final_acc = correct/float(len(test_loader))
    print("\nEpsilon: {}\tTest Accuracy = {} / {} = {}%\n".format(epsilon, correct, len(test_loader), final_acc*100))

    # Return the accuracy and an adversarial example
    return final_acc, adv_examples, adv_samples, sample_labels

epsilons = [0.1, .2, .4, .6, .8, 1.0]

"""# FGSM ATTACK: NET A"""

# ATTACK NET 1
accuracies = []
examples = []

# Run test for each epsilon
for eps in epsilons:
    acc, ex, Net_1_adv_samples, sample_labels = test(Net_1, device, test_loader, eps)
    accuracies.append(acc)
    examples.append(ex)

    
    transform_norm = transforms.Normalize((0.1307,), (0.3081,))
    dataloader = []
    for i, target in enumerate(sample_labels):
      adv_sample = transform_norm(Net_1_adv_samples[i][0])
      dataloader.append((adv_sample.unsqueeze(dim=0), target))

    # ATTACK NET 2  
    evaluate_model(Net_2, dataloader, tag='Net_2', Pretrained=None)

    # ATTACK NET 3
    evaluate_model(Net_3, dataloader, tag='Net_3', Pretrained=None)

    # ATTACK NET 4
    evaluate_model(Net_4, dataloader, tag='Net_4', Pretrained=None)

plt.figure(figsize=(5,5))
plt.plot(epsilons, accuracies, "*-")
plt.yticks(np.arange(0, 1.1, step=0.1))
plt.xticks(np.arange(0, 1.1, step=0.25))
plt.title("Accuracy vs Epsilon")
plt.xlabel("Epsilon")
plt.ylabel("Accuracy")
plt.show()

# Plot several examples of adversarial samples at each epsilon
cnt = 0
plt.figure(figsize=(8,10))
for i in range(len(epsilons)):
    for j in range(len(examples[i])):
        cnt += 1
        plt.subplot(len(epsilons),len(examples[0]),cnt)
        plt.xticks([], [])
        plt.yticks([], [])
        if j == 0:
            plt.ylabel("Eps: {}".format(epsilons[i]), fontsize=14)
        orig,adv,ex = examples[i][j]
        plt.title("{} -> {}".format(orig, adv))
        plt.imshow(ex, cmap="gray")
plt.tight_layout()
plt.show()

"""# FGSM ATTACK: NET A'"""

# ATTACK NET 2
accuracies = []
examples = []

# Run test for each epsilon
for eps in epsilons:
    acc, ex, Net_2_adv_samples, sample_labels = test(Net_2, device, test_loader, eps)
    accuracies.append(acc)
    examples.append(ex)

    
    transform_norm = transforms.Normalize((0.1307,), (0.3081,))
    dataloader = []
    for i, target in enumerate(sample_labels):
      adv_sample = transform_norm(Net_2_adv_samples[i][0])
      dataloader.append((adv_sample.unsqueeze(dim=0), target))

    # ATTACK NET 1  
    evaluate_model(Net_1, dataloader, tag='Net_1', Pretrained=None)

    # ATTACK NET 3
    evaluate_model(Net_3, dataloader, tag='Net_3', Pretrained=None)

    # ATTACK NET 4
    evaluate_model(Net_4, dataloader, tag='Net_4', Pretrained=None)

plt.figure(figsize=(5,5))
plt.plot(epsilons, accuracies, "*-")
plt.yticks(np.arange(0, 1.1, step=0.1))
plt.xticks(np.arange(0, 1.1, step=0.25))
plt.title("Accuracy vs Epsilon")
plt.xlabel("Epsilon")
plt.ylabel("Accuracy")
plt.show()

# Plot several examples of adversarial samples at each epsilon
cnt = 0
plt.figure(figsize=(8,10))
for i in range(len(epsilons)):
    for j in range(len(examples[i])):
        cnt += 1
        plt.subplot(len(epsilons),len(examples[0]),cnt)
        plt.xticks([], [])
        plt.yticks([], [])
        if j == 0:
            plt.ylabel("Eps: {}".format(epsilons[i]), fontsize=14)
        orig,adv,ex = examples[i][j]
        plt.title("{} -> {}".format(orig, adv))
        plt.imshow(ex, cmap="gray")
plt.tight_layout()
plt.show()

"""# FGSM ATTACK: NET B"""

# ATTACK NET 3
accuracies = []
examples = []

# Run test for each epsilon
for eps in epsilons:
    acc, ex, Net_3_adv_samples, sample_labels = test(Net_3, device, test_loader, eps)
    accuracies.append(acc)
    examples.append(ex)

    
    transform_norm = transforms.Normalize((0.1307,), (0.3081,))
    dataloader = []
    for i, target in enumerate(sample_labels):
      adv_sample = transform_norm(Net_3_adv_samples[i][0])
      dataloader.append((adv_sample.unsqueeze(dim=0), target))

    # ATTACK NET 1  
    evaluate_model(Net_1, dataloader, tag='Net_1', Pretrained=None)

    # ATTACK NET 2
    evaluate_model(Net_2, dataloader, tag='Net_2', Pretrained=None)

    # ATTACK NET 4
    evaluate_model(Net_4, dataloader, tag='Net_4', Pretrained=None)

plt.figure(figsize=(5,5))
plt.plot(epsilons, accuracies, "*-")
plt.yticks(np.arange(0, 1.1, step=0.1))
plt.xticks(np.arange(0, 1.1, step=0.25))
plt.title("Accuracy vs Epsilon")
plt.xlabel("Epsilon")
plt.ylabel("Accuracy")
plt.show()

# Plot several examples of adversarial samples at each epsilon
cnt = 0
plt.figure(figsize=(8,10))
for i in range(len(epsilons)):
    for j in range(len(examples[i])):
        cnt += 1
        plt.subplot(len(epsilons),len(examples[0]),cnt)
        plt.xticks([], [])
        plt.yticks([], [])
        if j == 0:
            plt.ylabel("Eps: {}".format(epsilons[i]), fontsize=14)
        orig,adv,ex = examples[i][j]
        plt.title("{} -> {}".format(orig, adv))
        plt.imshow(ex, cmap="gray")
plt.tight_layout()
plt.show()

"""# FGSM ATTACK: NET B'"""

# ATTACK NET 4
accuracies = []
examples = []

# Run test for each epsilon
for eps in epsilons:
    acc, ex, Net_4_adv_samples, sample_labels = test(Net_4, device, test_loader, eps)
    accuracies.append(acc)
    examples.append(ex)

    
    transform_norm = transforms.Normalize((0.1307,), (0.3081,))
    dataloader = []
    for i, target in enumerate(sample_labels):
      adv_sample = transform_norm(Net_4_adv_samples[i][0])
      dataloader.append((adv_sample.unsqueeze(dim=0), target))

    # ATTACK NET 1  
    evaluate_model(Net_1, dataloader, tag='Net_1', Pretrained=None)

    # ATTACK NET 2
    evaluate_model(Net_2, dataloader, tag='Net_2', Pretrained=None)

    # ATTACK NET 3
    evaluate_model(Net_3, dataloader, tag='Net_3', Pretrained=None)

plt.figure(figsize=(5,5))
plt.plot(epsilons, accuracies, "*-")
plt.yticks(np.arange(0, 1.1, step=0.1))
plt.xticks(np.arange(0, 1.1, step=0.25))
plt.title("Accuracy vs Epsilon")
plt.xlabel("Epsilon")
plt.ylabel("Accuracy")
plt.show()

# Plot several examples of adversarial samples at each epsilon
cnt = 0
plt.figure(figsize=(8,10))
for i in range(len(epsilons)):
    for j in range(len(examples[i])):
        cnt += 1
        plt.subplot(len(epsilons),len(examples[0]),cnt)
        plt.xticks([], [])
        plt.yticks([], [])
        if j == 0:
            plt.ylabel("Eps: {}".format(epsilons[i]), fontsize=14)
        orig,adv,ex = examples[i][j]
        plt.title("{} -> {}".format(orig, adv))
        plt.imshow(ex, cmap="gray")
plt.tight_layout()
plt.show()
